{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_transformers import BertConfig, BertTokenizer, BertModel\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=1):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 1\n",
    "model = BertForSequenceClassification(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/home/yinterian/data/aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bro', '##m', '##well', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = PATH/\"train/pos/0_9.txt\"\n",
    "z = tokenizer.tokenize(path.read_text())\n",
    "z[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22953, 2213, 4381, 2152, 2003, 1037, 9476, 4038, 1012, 2009]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(z)\n",
    "ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5909]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these tutorials\n",
    "* https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/\n",
    "* https://github.com/huggingface/pytorch-transformers/blob/master/README.md\n",
    "* https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "* https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2ids(text, max_seq_length=300):\n",
    "    tok_text = tokenizer.tokenize(text)\n",
    "    if len(tok_text) > max_seq_length:\n",
    "            tok_text = tok_text[:max_seq_length]\n",
    "    ids_text  = tokenizer.convert_tokens_to_ids(tok_text)\n",
    "    padding = [0] * (max_seq_length - len(ids_text))\n",
    "    ids_text += padding\n",
    "    return np.array(ids_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22953,  2213,  4381,  2152,  2003,  1037,  9476,  4038,  1012,\n",
       "        2009,  2743,  2012,  1996,  2168,  2051,  2004,  2070,  2060,\n",
       "        3454,  2055,  2082,  2166,  1010,  2107,  2004,  1000,  5089,\n",
       "        1000,  1012,  2026,  3486,  2086,  1999,  1996,  4252,  9518,\n",
       "        2599,  2033,  2000,  2903,  2008, 22953,  2213,  4381,  2152,\n",
       "        1005,  1055, 18312,  2003,  2172,  3553,  2000,  4507,  2084,\n",
       "        2003,  1000,  5089,  1000,  1012,  1996, 25740,  2000,  5788,\n",
       "       13732,  1010,  1996, 12369,  3993,  2493,  2040,  2064,  2156,\n",
       "        2157,  2083,  2037, 17203,  5089,  1005, 13433,  8737,  1010,\n",
       "        1996,  9004, 10196,  4757,  1997,  1996,  2878,  3663,  1010,\n",
       "        2035, 10825,  2033,  1997,  1996,  2816,  1045,  2354,  1998,\n",
       "        2037,  2493,  1012,  2043,  1045,  2387,  1996,  2792,  1999,\n",
       "        2029,  1037,  3076,  8385,  2699,  2000,  6402,  2091,  1996,\n",
       "        2082,  1010,  1045,  3202,  7383,  1012,  1012,  1012,  1012,\n",
       "        1012,  1012,  1012,  1012,  1012,  2012,  1012,  1012,  1012,\n",
       "        1012,  1012,  1012,  1012,  1012,  1012,  1012,  2152,  1012,\n",
       "        1037,  4438,  2240,  1024,  7742,  1024,  1045,  1005,  1049,\n",
       "        2182,  2000, 12803,  2028,  1997,  2115,  5089,  1012,  3076,\n",
       "        1024,  6160,  2000, 22953,  2213,  4381,  2152,  1012,  1045,\n",
       "        5987,  2008,  2116,  6001,  1997,  2026,  2287,  2228,  2008,\n",
       "       22953,  2213,  4381,  2152,  2003,  2521, 18584,  2098,  1012,\n",
       "        2054,  1037, 12063,  2008,  2009,  3475,  1005,  1056,   999,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ids(path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, PATH, train=\"train\"):\n",
    "        self.path_to_images = PATH/train\n",
    "        self.pos_files = list((self.path_to_images/\"pos\").iterdir())\n",
    "        self.neg_files = list((self.path_to_images/\"neg\").iterdir()) \n",
    "        self.files = self.pos_files + self.neg_files\n",
    "        self.y = np.concatenate((np.ones(len(self.pos_files), dtype=int),\n",
    "                                np.zeros(len(self.neg_files), dtype=int)), axis=0)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        path = self.files[index]\n",
    "        x = text2ids(path.read_text())\n",
    "        return x, self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImdbDataset(PATH)\n",
    "valid_ds = ImdbDataset(PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2004, 15444,  2890,  1998,  7369,  2012,  1996,  4578,  1997,  2037,\n",
       "         6217,  1012,  1999,  4266,  4841,  2245,  1997,  1996,  3212,  2004,\n",
       "         1037,  2173,  2005,  2299,  1998,  3153,  1012, 25755,  2001,  2145,\n",
       "         1037,  2261,  2086,  2185,  1012,  5965,  1998, 14580,  3153,  2039,\n",
       "         1996,  2237,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "         1028,  1996,  5436,  2003, 11519,  1010,  2021,  2040, 14977,  1012,\n",
       "         1012,  1012,  2011,  1996,  2126,  1010,  5060,  1996, 12081,  4395,\n",
       "         2005,  9306,  6723,  2571,  1998,  1037,  1043, 10278, 25373, 12776,\n",
       "         9463,  3608,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "         1028,  1037,  7170,  1997, 12415,  4068,  2774,  1010,  2164,  1996,\n",
       "         3297,  1000,  2292,  1005,  1055,  2227,  1996,  2189,  1998,  3153,\n",
       "         1000,  1012,  1999,  2008,  3496,  1010, 14580,  1005,  1055,  3082,\n",
       "        25430, 29046,  4377, 21526,  2015,  5965,  1999,  1996,  2227,  2076,\n",
       "         2028,  1997,  2014, 23371,  1998,  2471, 21145,  2032,  9787,  1012,\n",
       "         5965,  7278,  2006,  4363,  1996,  2202,  2004,  1996,  5613,  2001,\n",
       "        21688,  9690,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "         1028, 14580,  2320,  7034,  2008,  2016,  2001,  1037,  2488,  8033,\n",
       "         2084,  5965,  1010,  2144,  2016,  2018,  2000,  2079,  2035,  1996,\n",
       "         2168,  5829,  1010,  1999,  3357,  1010,  1998, 11043,  1012,  1012,\n",
       "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2272,\n",
       "         2000,  2228,  1997,  2009,  1010,  5965,  1005,  1055,  2376,  2001,\n",
       "         3835,  2205,  1012,  1996,  2158,  2001,  3947,  3238,  1999,  4367,\n",
       "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2182,\n",
       "         1005,  1055,  1037,  3185,  2000, 26931,  2039,  2006,  1996,  6411,\n",
       "         2007,  1037,  3866,  1011,  2028,  1010,  5926,  2125,  1996,  6007,\n",
       "         1010,  1998,  5959,  1996,  4024,  1012,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_dl:\n",
    "            x = x.cuda()\n",
    "            y = y.unsqueeze(1).float().cuda()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, y)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * x.size(0)\n",
    "        epoch_loss = running_loss / len(train_ds)\n",
    "        val_loss, accuracy = eval_model(model)\n",
    "        print('train loss: {:.3f}, valid loss {:.3f} accuracy {:.3f}'.format(\n",
    "            epoch_loss, val_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for x, y in valid_dl:\n",
    "        x = x.cuda()\n",
    "        y = y.unsqueeze(1).float().cuda()\n",
    "        logits = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y) \n",
    "        y_pred = logits > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    accuracy = correct / len(valid_ds)\n",
    "    epoch_loss = running_loss / len(valid_ds)\n",
    "    return epoch_loss, accuracy.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrlast = .0001\n",
    "lrmain = .00001\n",
    "optimizer = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.286, valid loss 0.201 accuracy 0.922\n",
      "train loss: 0.166, valid loss 0.210 accuracy 0.922\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
